{
  "configuration": "Konfigurasi",
  "model": "Model",
  "token": {
    "label": "Token Maksimum",
    "description": "Jumlah token maksimum untuk dijana dalam lengkapan sembang. Panjang keseluruhan token input dan token yang dijana adalah terhad oleh panjang konteks model."
  },
  "default": "Lalai",
  "temperature": {
    "label": "Suhu",
    "description": "Suhu pensampelan yang digunakan, antara 0 dan 2. Nilai yang lebih tinggi seperti 0.8 akan menjadikan keluaran lebih rawak, manakala nilai yang lebih rendah seperti 0.2 akan menjadikannya lebih terarah dan deterministik. Kami secara umumnya mengesyorkan mengubah ini atau atas p tetapi bukan kedua-duanya. (Lalai: 1)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Nombor antara 0 dan 1. Alternatif kepada pensampelan dengan suhu, dipanggil pensampelan nukleus, di mana model mempertimbangkan hasil token dengan jisim kebarangkalian top p. Jadi 0.1 bermaksud hanya token yang terdiri daripada 10% jisim kebarangkalian teratas dipertimbangkan. Secara umumnya, kami mengesyorkan untuk mengubah suhu atau nilai paling atas tetapi bukan kedua-duanya. (Lalai: 1)"
  },
  "defaultChatConfig": "Konfigurasi Cakap Lalai",
  "defaultSystemMessage": "Mesej Sistem Lalai",
  "resetToDefault": "Set Semula ke Lalai"
}
