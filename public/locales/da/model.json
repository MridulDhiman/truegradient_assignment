{
  "configuration": "Konfiguration",
  "model": "Model",
  "token": {
    "label": "Max Token",
    "description": "Det maksimale antal tokens der skal genereres i chat-fuldførelsen. Den samlede længde af input tokens og genererede tokens er begrænset af modellens kontekstlængde."
  },
  "default": "Standard",
  "temperature": {
    "label": "Temperatur",
    "description": "Hvilken samplingstemperatur der skal bruges, mellem 0 og 2. Højere værdier som 0,8 vil gøre outputtet mere tilfældigt, mens lavere værdier som 0,2 vil gøre det mere fokuseret og deterministisk. Vi anbefaler generelt at ændre dette eller top p, men ikke begge. (Standard: 1)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Tal mellem 0 og 1. Et alternativ til prøvetagning med temperatur, kaldet nucleus sampling, hvor modellen overvejer resultaterne af tokens med top p sandsynlighedsmasse. Så 0,1 betyder, at kun tokens, der udgør de øverste 10% sandsynlighedsmasse, overvejes. Vi anbefaler generelt at ændre dette eller temperaturen, men ikke begge. (Standard: 1)"
  },
  "defaultChatConfig": "Standard Chat-konfiguration",
  "defaultSystemMessage": "Standard Systembesked",
  "resetToDefault": "Nulstil til standard"
}
