{
  "configuration": "Configurazione",
  "model": "Modello",
  "token": {
    "label": "Token Massimo",
    "description": "Il numero massimo di token da generare nel completamento della chat. La lunghezza totale dei token in ingresso e di quelli generati è limitata dalla lunghezza del contesto del modello."
  },
  "default": "Default",
  "temperature": {
    "label": "Temperatura",
    "description": "Quale temperatura di campionamento utilizzare, tra 0 e 2. Valori più alti, come 0,8, renderanno l'output più casuale, mentre valori più bassi, come 0,2, lo renderanno più mirato e deterministico. In genere si consiglia di modificare questo valore o quello superiore, ma non entrambi. (Valore predefinito: 1)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Numero compreso tra 0 e 1. Un'alternativa al campionamento con temperatura, chiamato campionamento del nucleo, in cui il modello considera i risultati dei token con la massa di probabilità p più alta. Quindi 0,1 significa che vengono considerati solo i token che comprendono il 10% della massa di probabilità. In genere si consiglia di modificare questo parametro o la temperatura, ma non entrambi. (Predefinito: 1)"
  },
  "defaultChatConfig": "Configurazione predefinita della conversazione",
  "defaultSystemMessage": "Messaggio di sistema predefinito",
  "resetToDefault": "Ripristina alle impostazioni predefinite"
}
